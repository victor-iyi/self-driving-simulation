"""Helper file for working with training dataset.

   @author 
     Victor I. Afolabi
     Artificial Intelligence & Software Engineer.
     Email: javafolabi@gmail.com
     GitHub: https://github.com/victor-iyiola
  
   @project
     File: data.py
     Created on 08 June, 2018 @ 8:27 PM.
  
   @license
     MIT License
     Copyright (c) 2018. Victor I. Afolabi. All rights reserved.
"""
import os

import numpy as np
import pandas as pd
import cv2

import tensorflow as tf

# Path to where training data is collected (from the simulator).
data_dir = os.path.dirname(os.path.curdir)
data_dir = os.path.join(data_dir, 'simulated_data')

# CSV File generated by the Simulator.
CSV_FILENAME = os.path.join(data_dir, 'driving_log.csv')
IMG_DIR = os.path.join(data_dir, 'IMG')
FILE_NAMES = ['img_center', 'img_left', 'img_right',
              'center', 'left', 'right', 'steering_angle']

img_size, img_depth = 32, 3


def _parser(filename: tf.string, label: tf.Tensor):
    # Reads an image from a file, decodes it into a dense tensor, and resizes it
    # to a fixed shape.
    image_string = tf.read_file(filename)
    image_decoded = tf.image.decode_image(image_string)
    image_resized = tf.image.resize_images(image_decoded, [img_size, img_size, img_depth])
    return image_resized, label


def make_dataset(features: np.ndarray, labels: np.ndarray = None, **kwargs):
    # Extract keyword arguments.
    shuffle = kwargs.get('shuffle') or True
    buffer_size = kwargs.get('buffer_size') or 1000
    batch_size = kwargs.get('batch_size') or 128

    # Read CSV file into dataset object.
    if labels is not None:
        dataset = tf.data.Dataset.from_tensor_slices((features, labels))
        dataset = dataset.map(_parser)
    else:
        dataset = tf.data.Dataset.from_tensor_slices(features)

    # Apply transformation steps...
    if shuffle:
        dataset = dataset.shuffle(buffer_size=buffer_size).repeat()
    dataset = dataset.batch(batch_size=batch_size)

    return dataset


def load_data(features: np.ndarray = None, **kwargs):
    if not features.tolist():
        # Read dataset from cvs file if there are no features.
        df = pd.read_csv(CSV_FILENAME, names=FILE_NAMES)

        # Extract features & labels.
        features = df.drop([FILE_NAMES[0]], axis=1).values
        labels = df[FILE_NAMES[-1]].astype(np.float32).values

        # Create a dataset object.
        dataset = make_dataset(features, labels, **kwargs)
    else:
        dataset = make_dataset(features, labels=None, **kwargs)

    return dataset
